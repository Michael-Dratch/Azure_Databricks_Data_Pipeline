{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4cbfb0d6-3839-4f3f-a951-783319bbe7b0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Functions for silver layer transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8bf75f1-8ad1-499d-a99e-fdc294c06588",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, countDistinct, isnull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34fe6088-83d6-4ee0-92d0-9617686f9d40",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Creates all event tables from a given dataframe and returns them as a dictionary with key {event_name} and value {dataframe containing those events}\n",
    "def create_event_tables(df_in):\n",
    "    events_df = remove_non_event_cols(df_in)#Remove actor/repo/org columns\n",
    "    event_names = df_in.select('type').distinct().rdd.map(lambda r: r[0]).collect()# get python list of unique event types (names)\n",
    "\n",
    "    #map over list of event names, returning a tuples (event_name, event_df) as list; See create_event_table function\n",
    "    event_tables_tuple = map(lambda e_name: (e_name, create_event_table(events_df, e_name)), event_names)\n",
    "\n",
    "    #convert list of tuples (event_name, event_df) to dictionary with event_name as key, and event_df as value\n",
    "    event_tables = {event_name:event_table for event_name,event_table in event_tables_tuple}\n",
    "    \n",
    "    return event_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9673ff5-455f-49bd-88fe-977042d06149",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Drops columns unrelated to event tables\n",
    "def remove_non_event_cols(df_in):\n",
    "    drop_column_names = ['actor_gravatar_id','actor_login','actor_url','org_avatar_url','org_gravatar_id','org_login',\n",
    "                        'org_url','repo_name','repo_url','actor_avatar','actor_display_login','actor_avatar']\n",
    "    return df_in.drop(*drop_column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6a9c0e3-2683-4bf7-8a93-3007c78b446c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Drop columns where a sample of 20 columns is all null (assume the column is all null). \n",
    "def drop_null_cols(df_in): \n",
    "    row = df_in.take(1)[0].asDict() # get one row, for schema as a dict\n",
    "    count = 20 #sample only a few rows for performance\n",
    "    drop_names = [] #list of names to drop\n",
    "    df = df_in.limit(20) #sample n number of rows\n",
    "    for name, value in row.items(): #iterate over rows\n",
    "        null_count = df.where(col(name).isNull()).count()\n",
    "        if count == null_count: #if all sampled rows are null, assume column is row\n",
    "            drop_names.append(name) #add this column to list of names to drop\n",
    "    df_out = df_in.drop(*drop_names) #drop the null columns\n",
    "    return df_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a80ed6f3-7b9d-4626-8854-11757c4206a6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Flattens the specified column and drops the original column\n",
    "def flatten_out(df_in, column_name):\n",
    "    #select all columns, and sub columns of event_name as one df, and drop event_name column\n",
    "    df_out = df_in.select('*', column_name+\".*\").drop(column_name)\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80d54b2f-78a3-4eaf-bfa6-516710db9731",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# filters rows of dataframe by event type string \n",
    "def filter_event_type(df_in, event_type = str):\n",
    "    event_df = df_in.where(df_in.type == f'{event_type}')\n",
    "    return(event_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d6f71f9-adfb-4cb8-9fdc-84d618189bc2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Creates a single event table\n",
    "def create_event_table(df_in, event_name):\n",
    "    #et all events of specific type (i.e. ForkEvent, PushEvent, etc.)\n",
    "    event_table = filter_event_type(df_in, event_name)\n",
    "\n",
    "    #this is used on paylod, such that the result is all of payload's columns in the event df, and no more payload column\n",
    "    event_table = flatten_out(event_table, \"payload\")\n",
    "\n",
    "    #drop null columns\n",
    "    event_table = drop_null_cols(event_table)\n",
    "    return event_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0f26c2d-e697-4d9d-b4d3-32e5b100e656",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Transformation_Functions",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
